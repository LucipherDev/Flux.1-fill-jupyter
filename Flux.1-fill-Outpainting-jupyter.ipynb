{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "#@markdown <center><h1>Install</h1></center>\n",
        "\n",
        "%cd /content\n",
        "!git clone -b totoro6 https://github.com/LucipherDev/ComfyUI /content/TotoroUI\n",
        "!git clone -b totoro https://github.com/LucipherDev/ComfyUI-GGUF /content/TotoroUI/custom_nodes/TotoroUI-GGUF\n",
        "!git clone -b totoro https://github.com/LucipherDev/ComfyUI-Inpaint-CropAndStitch /content/TotoroUI/custom_nodes/TotoroUI-Inpaint-CropAndStitch\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post2\n",
        "!pip install -q -r /content/TotoroUI/custom_nodes/TotoroUI-GGUF/requirements.txt\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "import nodes\n",
        "\n",
        "if not nodes.load_custom_node(\"custom_nodes/TotoroUI-GGUF\"):\n",
        "  raise Exception(\"Failed to load GGUF custom node\")\n",
        "\n",
        "if not nodes.load_custom_node(\"custom_nodes/TotoroUI-Inpaint-CropAndStitch\"):\n",
        "  raise Exception(\"Failed to load Inpaint-CropAndStitch custom node\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k3aTOrdb8HxC"
      },
      "outputs": [],
      "source": [
        "#@markdown <center><h1>Load Models</h1></center>\n",
        "\n",
        "import torch\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_differential_diffusion\n",
        "\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UnetLoaderGGUF = NODE_CLASS_MAPPINGS[\"UnetLoaderGGUF\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "DifferentialDiffusion = nodes_differential_diffusion.NODE_CLASS_MAPPINGS[\"DifferentialDiffusion\"]()\n",
        "\n",
        "print(f\"Downloading Flux1-fill-dev-Q4_K_S...\")\n",
        "!aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M https://huggingface.co/YarvixPA/FLUX.1-Fill-dev-gguf/resolve/main/flux1-fill-dev-Q4_K_S.gguf -d /content/TotoroUI/models/unet -o flux1-fill-dev-Q4_K_S.gguf\n",
        "\n",
        "print(\"Downloading VAE...\")\n",
        "!aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n",
        "\n",
        "print(\"Downloading Clips...\")\n",
        "!aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.safetensors\n",
        "!aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "\n",
        "with torch.inference_mode():\n",
        "    print(\"Loading Clips...\")\n",
        "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "    print(\"Loading VAE...\")\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "    print(f\"Loading Flux1-fill-dev-Q4_K_S...\")\n",
        "    unet = UnetLoaderGGUF.load_unet(f\"flux1-fill-dev-Q4_K_S.gguf\")[0]\n",
        "    unet = DifferentialDiffusion.apply(unet)[0]\n",
        "\n",
        "    unet_f, clip_f = unet, clip\n",
        "\n",
        "print(\"All Models Loaded!\")\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from google.colab import files, output\n",
        "from IPython.display import HTML, display\n",
        "from PIL import Image, ImageOps\n",
        "import io\n",
        "import base64\n",
        "\n",
        "import nodes\n",
        "from totoro_extras import nodes_flux\n",
        "from totoro_extras import nodes_mask\n",
        "from totoro import model_management\n",
        "\n",
        "CLIPTextEncodeFlux = nodes_flux.NODE_CLASS_MAPPINGS[\"CLIPTextEncodeFlux\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "LoadImage = NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
        "ImagePadForOutpaint = NODE_CLASS_MAPPINGS[\"ImagePadForOutpaint\"]()\n",
        "InpaintModelConditioning = NODE_CLASS_MAPPINGS[\"InpaintModelConditioning\"]()\n",
        "KSampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\n",
        "ImageToMask = nodes_mask.NODE_CLASS_MAPPINGS[\"ImageToMask\"]()\n",
        "MaskToImage = nodes_mask.NODE_CLASS_MAPPINGS[\"MaskToImage\"]()\n",
        "GrowMask = nodes_mask.NODE_CLASS_MAPPINGS[\"GrowMask\"]()\n",
        "InpaintCrop = NODE_CLASS_MAPPINGS[\"InpaintCrop\"]()\n",
        "InpaintStitch = NODE_CLASS_MAPPINGS[\"InpaintStitch\"]()\n",
        "InpaintExtendOutpaint = NODE_CLASS_MAPPINGS[\"InpaintExtendOutpaint\"]()\n",
        "ImageCompositeMasked = nodes_mask.NODE_CLASS_MAPPINGS[\"ImageCompositeMasked\"]()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown <center><h1>Functions</h1></center>\n",
        "\n",
        "interface = \"\"\"\n",
        "<style>#currentValues,.button{color:#fff;font-weight:700}body{background-color:#2d2d2d;font-family:Arial,sans-serif;color:#fff;margin:0;display:flex;flex-direction:column;align-items:center;justify-content:center}#buttonContainer,#toolbar{display:flex;flex-wrap:wrap;justify-content:center;align-items:center;margin:10px auto;background-color:#3c3f41;border-radius:8px;width:1024px;padding:10px 0;box-shadow:0 4px 10px rgba(0,0,0,.3)}.button{padding:10px 15px;margin:5px;font-size:14px;cursor:pointer;border-radius:5px;background-color:#5e5e5e;border:0;transition:.3s}.button:hover{background-color:#757575}.slider-container{display:flex;align-items:center;margin:10px 0;width:100%;padding:0 15px}.slider-container label{margin-right:10px}.slider-container input[type=range]{flex-grow:1;cursor:pointer}#currentValues{margin:15px 0;padding:10px;text-align:center}</style>\n",
        "<div id=\"buttonContainer\">\n",
        "  <button class=\"button\" id=\"resetButton\">Reset</button>\n",
        "  <button class=\"button\" id=\"updateButton\">Update</button>\n",
        "</div>\n",
        "<div id=\"toolbar\">\n",
        "  <div class=\"slider-container\">\n",
        "    <label for=\"topSlider\">Top:</label>\n",
        "    <input id=\"topSlider\" type=\"range\" min=\"0\" max=\"1024\" value=\"0\">\n",
        "  </div>\n",
        "  <div class=\"slider-container\">\n",
        "    <label for=\"leftSlider\">Left:</label>\n",
        "    <input id=\"leftSlider\" type=\"range\" min=\"0\" max=\"1024\" value=\"0\">\n",
        "  </div>\n",
        "  <div class=\"slider-container\">\n",
        "    <label for=\"bottomSlider\">Bottom:</label>\n",
        "    <input id=\"bottomSlider\" type=\"range\" min=\"0\" max=\"1024\" value=\"0\">\n",
        "  </div>\n",
        "  <div class=\"slider-container\">\n",
        "    <label for=\"rightSlider\">Right:</label>\n",
        "    <input id=\"rightSlider\" type=\"range\" min=\"0\" max=\"1024\" value=\"0\">\n",
        "  </div>\n",
        "  <div class=\"slider-container\">\n",
        "    <label for=\"featherSlider\">Feather:</label>\n",
        "    <input id=\"featherSlider\" type=\"range\" min=\"0\" max=\"512\" value=\"0\">\n",
        "  </div>\n",
        "</div>\n",
        "<div id=\"currentValues\">Current Values: Top:0, Left:0, Bottom:0, Right:0, Feather:0</div>\n",
        "<script>const sliders={top:document.getElementById(\"topSlider\"),left:document.getElementById(\"leftSlider\"),bottom:document.getElementById(\"bottomSlider\"),right:document.getElementById(\"rightSlider\"),feather:document.getElementById(\"featherSlider\")},resetButton=document.getElementById(\"resetButton\"),updateButton=document.getElementById(\"updateButton\"),currentValues=document.getElementById(\"currentValues\");let settings={top:0,left:0,bottom:0,right:0,feather:0};function updateSettings(){settings.top=parseInt(sliders.top.value),settings.left=parseInt(sliders.left.value),settings.bottom=parseInt(sliders.bottom.value),settings.right=parseInt(sliders.right.value),settings.feather=parseInt(sliders.feather.value),updateCurrentValues()}function updateCurrentValues(){currentValues.textContent=`Current Values: Top:${settings.top}, Left:${settings.left}, Bottom:${settings.bottom}, Right:${settings.right}, Feather:${settings.feather}`}function resetSettings(){Object.values(sliders).forEach((t=>t.value=0)),updateSettings()}function saveSettings(){google.colab.kernel.invokeFunction(\"notebook.update\",[settings.top,settings.left,settings.bottom,settings.right,settings.feather],{})}Object.values(sliders).forEach((t=>t.addEventListener(\"input\",updateSettings))),resetButton.addEventListener(\"click\",resetSettings),updateButton.addEventListener(\"click\",saveSettings),updateSettings();</script>\n",
        "\"\"\"\n",
        "\n",
        "top = 0\n",
        "left = 0\n",
        "bottom = 0\n",
        "right = 0\n",
        "feather = 0\n",
        "\n",
        "# @markdown <ul><li><h2>Outpaint Mask</h2></ul>\n",
        "\n",
        "input_image = \"/content/test.png\" # @param {\"type\":\"string\"}\n",
        "\n",
        "# @markdown <ul><li><h2>Crop and Stitch</h2></li></li></ul>\n",
        "\n",
        "context_expand_pixels = 20 # @param {\"type\":\"slider\",\"min\":0,\"max\":256,\"step\":1}\n",
        "context_expand_factor = 1 # @param {\"type\":\"slider\",\"min\":1,\"max\":100,\"step\":0.01}\n",
        "crop_mode = \"ranged size\" # @param [\"ranged size\", \"forced size\", \"free size\"] {\"type\":\"string\"}\n",
        "rescale_algorithm = \"bislerp\" # @param [\"nearest\", \"bilinear\", \"bicubic\", \"bislerp\", \"lanczos\", \"box\", \"hamming\"]  {\"type\":\"string\"}\n",
        "force_width =  1024 # @param {\"type\":\"slider\",\"min\":512,\"max\":1024,\"step\":1}\n",
        "force_height =  1024 # @param {\"type\":\"slider\",\"min\":512,\"max\":1024,\"step\":1}\n",
        "rescale_factor = 1 # @param {\"type\":\"slider\",\"min\":0.01,\"max\":100,\"step\":0.01}\n",
        "padding = 8 # @param [\"8\",\"16\",\"32\",\"64\",\"128\",\"256\",\"512\"] {\"type\":\"raw\"}\n",
        "min_width = 512 # @param {\"type\":\"slider\",\"min\":256,\"max\":512,\"step\":1}\n",
        "min_height = 512 # @param {\"type\":\"slider\",\"min\":256,\"max\":512,\"step\":1}\n",
        "max_width = 768 # @param {\"type\":\"slider\",\"min\":768,\"max\":1024,\"step\":1}\n",
        "max_height = 768 # @param {\"type\":\"slider\",\"min\":768,\"max\":1024,\"step\":1}\n",
        "\n",
        "def update(top_value, left_value, bottom_value, right_value, feather_value):\n",
        "    global top, left, bottom, right, feather\n",
        "    top = top_value\n",
        "    left = left_value\n",
        "    bottom = bottom_value\n",
        "    right = right_value\n",
        "    feather = feather_value\n",
        "    print(f\"Values updated: Top:{top}, Left:{left}, Bottom:{bottom}, Right:{right}, Feather:{feather}\")\n",
        "\n",
        "def resize_image(img, limit, resize_by):\n",
        "    width, height = img.size\n",
        "    longest = max(width, height)\n",
        "    shortest = min(width, height)\n",
        "\n",
        "    if resize_by ==\"longest\" and longest > limit:\n",
        "        scale_factor = limit / float(longest)\n",
        "        new_width = int(width * scale_factor)\n",
        "        new_height = int(height * scale_factor)\n",
        "        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "    elif resize_by == \"shortest\" and shortest > limit:\n",
        "        scale_factor = limit / float(shortest)\n",
        "        new_width = int(width * scale_factor)\n",
        "        new_height = int(height * scale_factor)\n",
        "        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "    return img\n",
        "\n",
        "def save_image(decoded, path, name, download=False):\n",
        "  full_path = os.path.abspath(os.path.join(path, name))\n",
        "  Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save( full_path)\n",
        "\n",
        "  img = Image.open(full_path)\n",
        "  display(img)\n",
        "\n",
        "  if download:\n",
        "    files.download(full_path)\n",
        "\n",
        "def img_tensor_to_np(img_tensor):\n",
        "  img_tensor = img_tensor.clone() * 255.0\n",
        "  return img_tensor.squeeze().numpy().astype(np.uint8)\n",
        "\n",
        "def img_np_to_tensor(img_np_list):\n",
        "  return torch.from_numpy(img_np_list.astype(np.float32) / 255.0).unsqueeze(0)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(pos_prompt, neg_prompt, fixed_seed, guidance, steps, cfg, sampler_name, scheduler, resize_before, size_limit, limit_by, use_crop_stitch, mask_expand, mask_blur, mask_blend, batch_size, auto_download):\n",
        "  global unet, clip, unet_f, clip_f\n",
        "\n",
        "  print(\"Prompt Received\")\n",
        "\n",
        "  pos_cond = CLIPTextEncodeFlux.encode(clip_f, pos_prompt, pos_prompt, guidance)[0]\n",
        "  neg_cond = CLIPTextEncodeFlux.encode(clip_f, neg_prompt, neg_prompt, guidance)[0]\n",
        "\n",
        "  image, mask = LoadImage.load_image(input_image)\n",
        "\n",
        "  if resize_before:\n",
        "    image_np = img_tensor_to_np(image)\n",
        "    img = Image.fromarray(image_np)\n",
        "\n",
        "    mask_image = MaskToImage.mask_to_image(mask)[0]\n",
        "    mask_np = img_tensor_to_np(mask_image)\n",
        "    mask_img = Image.fromarray(mask_np)\n",
        "\n",
        "    img = resize_image(img, size_limit, limit_by)\n",
        "    mask_img = resize_image(mask_img, size_limit, limit_by)\n",
        "\n",
        "    image = img_np_to_tensor(np.array(img))\n",
        "    mask = ImageToMask.image_to_mask(img_np_to_tensor(np.array(mask_img)), \"red\")[0]\n",
        "\n",
        "  if use_crop_stitch:\n",
        "    image, mask, _ = InpaintExtendOutpaint.inpaint_extend(image, mask, \"pixels\", top, 0, bottom, 0, left, 0, left, 0, mask)\n",
        "\n",
        "    mask = GrowMask.expand_mask(mask, mask_expand, True)[0]\n",
        "\n",
        "    stitch, image, mask = InpaintCrop.inpaint_crop_single_image(image, mask, context_expand_pixels, context_expand_factor, False, mask_blur, False, mask_blend, crop_mode, rescale_algorithm, force_width, force_height, rescale_factor, padding, min_width, min_height, max_width, max_height)\n",
        "\n",
        "  else:\n",
        "    image, mask = ImagePadForOutpaint.expand_image(image, left, top, right, bottom, feather)\n",
        "\n",
        "  display(HTML(f\"\"\"\n",
        "    <div style=\"display: flex; gap: 10px;\">\n",
        "        {\"\".join(f'<img src=\"data:image/png;base64,{base64.b64encode(io.BytesIO(Image.fromarray(img).save((buf:=io.BytesIO()), format=\"PNG\") or buf.getvalue()).getvalue()).decode(\"utf-8\")}\" style=\"width: 512px;\">' for img in [img_tensor_to_np(image), img_tensor_to_np(MaskToImage.mask_to_image(mask)[0])])}\n",
        "    </div>\n",
        "  \"\"\"))\n",
        "\n",
        "  pos_cond, neg_cond, latent_image = InpaintModelConditioning.encode(pos_cond, neg_cond, image, vae, mask, True)\n",
        "\n",
        "  for i in range(0, batch_size):\n",
        "    if fixed_seed == 0:\n",
        "      seed = random.randint(0, 18446744073709551615)\n",
        "    else:\n",
        "      seed = fixed_seed\n",
        "\n",
        "    print(\"Seed:\", seed)\n",
        "\n",
        "    sample = KSampler.sample(unet_f, seed, steps, cfg, sampler_name, scheduler, pos_cond, neg_cond, latent_image)[0]\n",
        "    model_management.soft_empty_cache()\n",
        "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "    decoded = ImageCompositeMasked.composite(image, decoded, 0, 0, True, mask)[0]\n",
        "\n",
        "    if use_crop_stitch:\n",
        "      decoded = InpaintStitch.inpaint_stitch_single_image(stitch, decoded, rescale_algorithm)[0]\n",
        "\n",
        "    save_image(decoded, \"/content\", f\"flux_fill_{seed}_{i}.png\", auto_download)\n",
        "\n",
        "\n",
        "output.register_callback('notebook.update', update)\n",
        "display(HTML(interface))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Xczoka5RgsO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><h1>Outpaint</h1></center>\n",
        "\n",
        "positive_prompt = \"\" # @param {\"type\":\"string\"}\n",
        "negative_prompt = \"watermark, text, ugly, deformed, low quality\" # @param {\"type\":\"string\"}\n",
        "fixed_seed = 0 # @param {\"type\":\"slider\",\"min\":0,\"max\":18446744073709552000,\"step\":1}\n",
        "guidance = 30 # @param {\"type\":\"slider\",\"min\":0,\"max\":50,\"step\":0.5}\n",
        "steps = 20 # @param {\"type\":\"slider\",\"min\":4,\"max\":50,\"step\":1}\n",
        "cfg = 1 # @param {\"type\":\"slider\",\"min\":0,\"max\":100,\"step\":0.1}\n",
        "resize_before = True # @param {\"type\":\"boolean\"}\n",
        "size_limit = 1024 # @param {\"type\":\"slider\",\"min\":256,\"max\":2048,\"step\":1}\n",
        "limit_by = \"shortest\" # @param [\"longest\",\"shortest\"]\n",
        "use_crop_stitch = True # @param {\"type\":\"boolean\"}\n",
        "mask_expand = 0 # @param {\"type\":\"slider\",\"min\":-256,\"max\":256,\"step\":1}\n",
        "mask_blur = 16 # @param {\"type\":\"slider\",\"min\":0,\"max\":64,\"step\":1}\n",
        "mask_blend = 16 # @param {\"type\":\"slider\",\"min\":0,\"max\":32,\"step\":1}\n",
        "batch_size = 1 # @param {\"type\":\"slider\",\"min\":1,\"max\":20,\"step\":1}\n",
        "sampler_name = \"euler\" # @param [\"euler\",\"heun\",\"heunpp2\",\"heunpp2\",\"dpm_2\",\"lms\",\"dpmpp_2m\",\"ipndm\",\"deis\",\"ddim\",\"uni_pc\",\"uni_pc_bh2\"]\n",
        "scheduler = \"simple\" # @param [\"normal\",\"sgm_uniform\",\"simple\",\"ddim_uniform\"]\n",
        "auto_download = False # @param {\"type\":\"boolean\"}\n",
        "\n",
        "generate(positive_prompt, negative_prompt, fixed_seed, guidance, steps, cfg, sampler_name, scheduler, resize_before, size_limit, limit_by, use_crop_stitch, mask_expand, mask_blur, mask_blend, batch_size, auto_download)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KM1TdZt4YifS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}